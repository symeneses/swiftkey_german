---
title: "mySwiftKey"
author: "Sandra Meneses"
date: "24 Januar 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, error = FALSE)
setwd('~/Data_Science/R/Tasks/SwiftKey')
sapply(list.files(pattern="[.]R$", path="R/", full.names=TRUE), source, echo = FALSE, verbose = FALSE, print.eval = FALSE)
```

# Project Swiftkey: German Corpora

The main goal of this project is to build a predictive text model. That means, I will use some data science, or to be more specific, NLP (Natural Language Processing) to predict the next words a user intends to type. This reduces the time a person needs to write a message.


1. Retrieving the data

Data is collected from multiple sources: Blogs, news and twitter. To make the process faster a sample is taken.

```{r eval = FALSE}
getData("Coursera-SwiftKey.zip")
# Statistics from the corpus
stats <- statText(c('blogs_org', 'news_org', 'twitter_org'))
# getSample has parameters porc_train, porc_test
getSample(10,2)
# Statistics from the sample to check the training data and test data.
stats_sample <- statText(c("train","test"))

```

2. Creating n-grams

Data is cleaned, tokenized, DTM(Document-Term Matrix) with n-grams from 0 to 4 are created.
N-grams are finally organized in matrixes with their frequency in descedent order. Matrices are used are they are faster than dataframes, this will facilitate the search or words.

```{r eval = FALSE}
# Create corpus
createCorpus("train")
# Create Document Text Matrix with n-grams to 4
createDTM("train",1)
createDTM("train",2)
createDTM("train",3)
createDTM("train",4)
# Create frequence of ngrams in descendent order 
createTF("train",1)
createTF("train",2)
createTF("train",3)
createTF("train",4)
# Create matrix with all ngrams with their respective frequencies
createTotalTF("train")
```


3. Modelling

The function __nextWords__ extract the last 3 words of the text and look the most likely next 3 words. First using 3 grams, then 2 grams, 1 gram, partial 2 grams and finally, the most common words in the corpora. When the text or a new sentence starts, the next word will be also the most common words in the language. 

Examples:

```{r}
text <- "ich"
nextWords(text)
text <- "ich bin ein"
nextWords(text)
text <- "ich "
nextWords(text)
```

Conclusions

- The task of language modeling has multiple applications and help us to understand language.
- The use of more n grams can help to improve accuracy but demands more memory and make the predictions slower.
- BOW allow us to get useful predictions of the next word.


Future improvements

- Use word embeddings like word2vec to get better predictions for words that are not in the corpora.
- Include corpora with part of the speech tags to take probabilities of sequences (for example Verb -> Object -> Preposition) into consideration.
- Implementation of Recurrent Neural Networks (RNN) to predict the next word.



